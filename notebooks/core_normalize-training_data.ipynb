{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Workflow: Normalize training data\n",
    "Purpose: The mean and standard deviation of the band values for each pixel in each satellite imagery within the training data AOI are calculated and used to normalize the training data. This final normalized set of band values will be eventually used to train the model.\n",
    "<br>\n",
    "<br>\n",
    "*Date: 10-31-2019*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/opt/caffe/python', '/opt/caffe2/build', '/data/home/peter/notebooks/urban_heat', '/anaconda/envs/py36/lib/python36.zip', '/anaconda/envs/py36/lib/python3.6', '/anaconda/envs/py36/lib/python3.6/lib-dynload', '/anaconda/envs/py36/lib/python3.6/site-packages', '/anaconda/envs/py36/lib/python3.6/site-packages/IPython/extensions', '/data/home/peter/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import itertools\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "#\n",
    "import numpy as np\n",
    "import shapely\n",
    "from shapely.geometry import shape, Point\n",
    "from shapely.geometry import mapping, Polygon\n",
    "# import cartopy\n",
    "import geojson\n",
    "import fiona\n",
    "import h5py\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gdal\n",
    "from glob import glob\n",
    "\n",
    "import jenkspy\n",
    "\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "from numpy import mean\n",
    "\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "import time\n",
    "\n",
    "import descarteslabs as dl\n",
    "print (sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shape(place_shapefile):\n",
    "    c = fiona.open(place_shapefile)\n",
    "    pol = c.next()\n",
    "    shape = {}\n",
    "    shape['type'] = pol['type']\n",
    "    shape['properties'] = pol['properties']\n",
    "    shape['geometry'] = {}\n",
    "    shape['geometry']['type'] = 'Polygon'  # pol['geometry']['type']\n",
    "    shape['geometry']['coordinates'] = [[]]\n",
    "    # if MultiPolygon (e.g., city='kampala')\n",
    "    if (len(pol['geometry']['coordinates'])>1):\n",
    "        # identify largest single polygon\n",
    "#         print(\"MultiPolygon\", len(pol['geometry']['coordinates']))\n",
    "        p_argmax = 0 \n",
    "        pn_max = 0\n",
    "        for p in range(len(pol['geometry']['coordinates'])):\n",
    "            pn = len(pol['geometry']['coordinates'][p][0])\n",
    "            if pn>pn_max:\n",
    "                p_argmax = p\n",
    "                pn_max = pn\n",
    "#             print(p, pn, p_argmax, pn_max )\n",
    "        # make largest polygon the only polygon, move other polys to a backup variable \n",
    "        polygon = pol['geometry']['coordinates'][p_argmax]\n",
    "    else:\n",
    "#         print('simple polygon')\n",
    "        polygon = pol['geometry']['coordinates']\n",
    "       \n",
    "    xmin =  180\n",
    "    xmax = -180\n",
    "    ymin =  90\n",
    "    ymax = -90\n",
    "    for x,y in polygon[0]:\n",
    "        xmin = xmin if xmin < x else x\n",
    "        xmax = xmax if xmax > x else x\n",
    "        ymin = ymin if ymin < y else y\n",
    "        ymax = ymax if ymax > y else y\n",
    "        shape['geometry']['coordinates'][0].append([x,y])\n",
    "    shape['bbox'] = [xmin,ymin,xmax,ymax]\n",
    "    \n",
    "    return shape\n",
    "\n",
    "import itertools\n",
    "from multiprocessing import Process, cpu_count\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#\n",
    "# CONFIG\n",
    "#\n",
    "MAX_POOL_PROCESSES=cpu_count()-1\n",
    "MAX_THREADPOOL_PROCESSES=16\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# METHODS\n",
    "#\n",
    "\"\"\" MAP METHODS\n",
    "  Args:\n",
    "    * map_function <function>: \n",
    "      a function to map over args list. the function should take a single argument.\n",
    "      if multiple arguments are needed accept them as a single list or tuple\n",
    "    * args_list <list>: the list of arguments to map over\n",
    "    * max_process <int>: number of processes\n",
    "      - for max_with_pool defaults to the number of cpus minus 1\n",
    "      - for max_with_threadpool defaults to 16\n",
    "      - map_sequential ignores this argument as its doesn't actually do \n",
    "        any multiprocesssing \n",
    "  Return:\n",
    "    List of return values from map_function\n",
    "  Notes:\n",
    "    map_sequential does NOT multiprocess.  it can be used as a sequential drop-in \n",
    "    replacement for map_with_pool/threadpool.  this is useful for:\n",
    "      - development \n",
    "      - debugging\n",
    "      - benchmarking \n",
    "\"\"\"\n",
    "def map_with_pool(map_function,args_list,max_processes=MAX_POOL_PROCESSES):\n",
    "  pool=Pool(processes=min(len(args_list),max_processes))\n",
    "  return _run_pool(pool,map_function,args_list)\n",
    "\n",
    "\n",
    "def map_with_threadpool(map_function,args_list,max_processes=MAX_THREADPOOL_PROCESSES):\n",
    "  pool=ThreadPool(processes=min(len(args_list),max_processes))\n",
    "  return _run_pool(pool,map_function,args_list)\n",
    "\n",
    "\n",
    "def map_sequential(map_function,args_list,print_args=False,noisy=False,**dummy_kwargs):\n",
    "  if noisy:\n",
    "    print('multiprocessing(test):')\n",
    "  out=[]\n",
    "  for i,args in enumerate(args_list):\n",
    "      if noisy: \n",
    "        print('\\t{}...'.format(i))\n",
    "      if print_args:\n",
    "        print('\\t{}'.format(args))\n",
    "      out.append(map_function(args))\n",
    "  if noisy: \n",
    "    print('-'*25)\n",
    "  return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" simple: vanilla multiprocessing\n",
    "  Args:\n",
    "    * function <function>: function. function can take multiple arguments \n",
    "    * args_list <list>: the list of argument lists\n",
    "    * join <bool[True]>: join processes before return\n",
    "  Return: \n",
    "    List of processes \n",
    "\"\"\"\n",
    "def simple(function,args_list,join=True):\n",
    "  procs=[]\n",
    "  for args in args_list:\n",
    "      proc=Process(\n",
    "          target=function, \n",
    "          args=args)\n",
    "      procs.append(proc)\n",
    "      proc.start()\n",
    "  if join:\n",
    "    for proc in procs:\n",
    "        proc.join()\n",
    "  return procs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" MPList\n",
    "Run the above methods on map_function,args_list pairs where the map_function\n",
    "changes for each new set of args in args_list\n",
    "Args:\n",
    "    pool_type<str>: \n",
    "        one of MPList.POOL|THREAD|SEQUENTIAL.  determines which map_function \n",
    "        and default max_processes to use. If not MPList.THREAD|SEQUENTIAL it \n",
    "        will default to MPList.POOL.\n",
    "    max_processes<int>:\n",
    "        if not passed will set default based on pool_type\n",
    "    jobs<list>:\n",
    "        list of (target,args,kwargs) tuples. Note: use the append method rather than\n",
    "        creating (target,args,kwargs) tuples\n",
    "        \n",
    "\"\"\"\n",
    "class MPList():\n",
    "    #\n",
    "    # POOL TYPES\n",
    "    #\n",
    "    POOL='pool'\n",
    "    THREAD='threading'\n",
    "    SEQUENTIAL='sequential'\n",
    "    \n",
    "\n",
    "    #\n",
    "    # PUBLIC\n",
    "    #\n",
    "    def __init__(self,pool_type=None,max_processes=None,jobs=None):\n",
    "        self.pool_type=pool_type or self.POOL\n",
    "        self.max_processes=max_processes\n",
    "        self.jobs=jobs or []\n",
    "\n",
    "        \n",
    "    def append(self,target,*args,**kwargs):\n",
    "        self.jobs.append((target,)+(args,)+(kwargs,))\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "        self.start_time=datetime.now()\n",
    "        map_func,self.max_processes=self._map_func_max_processes()\n",
    "        out=map_func(self._target,self.jobs,max_processes=self.max_processes)\n",
    "        self.end_time=datetime.now()\n",
    "        self.duration=str(self.end_time-self.start_time)\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.jobs)\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # INTERNAL\n",
    "    #    \n",
    "    def _map_func_max_processes(self):\n",
    "        if self.pool_type==MPList.THREAD:\n",
    "            map_func=map_with_threadpool\n",
    "            max_processes=self.max_processes or MAX_THREADPOOL_PROCESSES\n",
    "        elif self.pool_type==MPList.SEQUENTIAL:\n",
    "            map_func=map_sequential\n",
    "            max_processes=False\n",
    "        else:\n",
    "            map_func=map_with_pool\n",
    "            max_processes=self.max_processes or MAX_POOL_PROCESSES\n",
    "        return map_func, max_processes\n",
    "        \n",
    "        \n",
    "    def _target(self,args):\n",
    "        target,args,kwargs=args\n",
    "        return target(*args,**kwargs)\n",
    "        \n",
    "    \n",
    "\n",
    "#\n",
    "# INTERNAL METHODS\n",
    "#\n",
    "def _stop_pool(pool,success=True):\n",
    "  pool.close()\n",
    "  pool.join()\n",
    "  return success\n",
    "\n",
    "\n",
    "def _map_async(pool,map_func,objects):\n",
    "  try:\n",
    "    return pool.map_async(map_func,objects)\n",
    "  except KeyboardInterrupt:\n",
    "    print(\"Caught KeyboardInterrupt, terminating workers\")\n",
    "    pool.terminate()\n",
    "    return False\n",
    "  else:\n",
    "    print(\"Failure\")\n",
    "    return _stop_pool(pool,False)\n",
    "\n",
    "\n",
    "def _run_pool(pool,map_function,args_list):\n",
    "  out=_map_async(pool,map_function,args_list)\n",
    "  _stop_pool(pool)\n",
    "  return out.get()\n",
    "\n",
    "\n",
    "#------------------------Scene Normalization------------------------------\n",
    "\n",
    "def normalize_scenes(scenes_list):\n",
    "    try:\n",
    "        scene, ctx = dl.scenes.Scene.from_id(scenes_list)\n",
    "        arr = scene.ndarray(bands=\"red green blue nir\", ctx=ctx.assign(resolution=50),mask_alpha=False)\n",
    "\n",
    "        red = arr[0]\n",
    "        red = red.astype(float)\n",
    "\n",
    "        green = arr[1]\n",
    "        green = green.astype(float)\n",
    "\n",
    "        blue = arr[2]\n",
    "        blue = blue.astype(float)\n",
    "\n",
    "        nir = arr[3]\n",
    "        nir = nir.astype(float)\n",
    "\n",
    "        for val in red:\n",
    "            for v in val:\n",
    "                red_means.append(v)\n",
    "        for val in green:\n",
    "            for v in val:\n",
    "                green_means.append(v)\n",
    "        for val in blue:\n",
    "            for v in val:\n",
    "                blue_means.append(v)\n",
    "        for val in nir:\n",
    "            for v in val:\n",
    "                nir_means.append(v)\n",
    "\n",
    "        print('correct')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your input file here\n",
    "path_data = 'band_values_train_data.csv'\n",
    "\n",
    "# Read the data to a Pandas Dataframe\n",
    "path_df = pd.read_csv(path_data, encoding='utf8')\n",
    "\n",
    "img_info= path_df[['img_path', 'footprint_shapes','latitude', 'longitude', 'tile_id',\n",
    "                   'roof_no', 'red_mean','green_mean','blue_mean','nir_mean','expected_albedo']].apply(tuple, axis=1)\n",
    "\n",
    "path_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of scene ids that cover the training data\n",
    "scene_ids = list(df['tile_id'])\n",
    "\n",
    "# Remove any duplicate scene ids\n",
    "scene_ids = list(dict.fromkeys(scene_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_means=[]\n",
    "green_means=[]\n",
    "blue_means=[]\n",
    "nir_means=[]\n",
    "\n",
    "\n",
    "%time out = map_with_threadpool(normalize_scenes,scene_ids,max_processes=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and standard deviation of the band values for each pixel within each tile that covers training data\n",
    "\n",
    "big_red_mean=mean(red_means)\n",
    "big_green_mean=mean(green_means)\n",
    "big_blue_mean=mean(blue_means)\n",
    "big_nir_mean=mean(nir_means)\n",
    "\n",
    "big_red_sd=statistics.stdev(red_means)\n",
    "big_green_sd=statistics.stdev(green_means)\n",
    "big_blue_sd=statistics.stdev(blue_means)\n",
    "big_nir_sd=statistics.stdev(nir_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize training data using the constants calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_r_m = []\n",
    "norm_g_m = []\n",
    "norm_b_m = []\n",
    "norm_n_m = []\n",
    "\n",
    "lat=[]\n",
    "lon=[]\n",
    "roofs = []\n",
    "\n",
    "tile_ids = []\n",
    "\n",
    "exp_values = []\n",
    "img_path = []\n",
    "\n",
    "footprint_shapes=[]\n",
    "\n",
    "for Y, X in img_info.iteritems():\n",
    "    \n",
    "    raw_red_mean=(X[6])\n",
    "    raw_green_mean=(X[7])\n",
    "    raw_blue_mean=(X[8])\n",
    "    raw_nir_mean=(X[9])\n",
    "\n",
    "    r = (raw_red_mean-big_red_m)/big_red_sd\n",
    "    g = (raw_green_mean-big_green_m)/big_green_sd\n",
    "    b = (raw_blue_mean-big_blue_m)/big_blue_sd\n",
    "    n = (raw_nir_mean-big_nir_m)/big_nir_sd\n",
    "\n",
    "    norm_r_m.append(r)\n",
    "    norm_g_m.append(g)\n",
    "    norm_b_m.append(b)\n",
    "    norm_n_m.append(n)    \n",
    "\n",
    "    raw_reds.append(raw_red_mean)\n",
    "    raw_greens.append(raw_green_mean)\n",
    "    raw_blues.append(raw_blue_mean)\n",
    "    raw_nirs.append(raw_nir_mean)\n",
    "\n",
    "    img_path.append(X[0])\n",
    "    roofs.append(X[5])\n",
    "    footprint_shapes.append(X[1])\n",
    "    tile_ids.append(X[4])\n",
    "    lat.append(X[2])\n",
    "    lon.append(X[3])\n",
    "    exp_values.append(X[10])\n",
    "    \n",
    "\n",
    "# store the results to a pandas library.\n",
    "df = pd.DataFrame({ 'img_path': img_path, 'roof_id':roofs, 'footprint_shapes':footprint_shapes,'tile_ids':tile_ids, \n",
    "                   'latitude':lat, 'longitude':lon,'norm_red_mean': norm_r_m,'norm_green_mean': norm_g_m,\n",
    "                   'norm_blue_mean': norm_b_m,'norm_nir_mean': norm_n_m,\n",
    "                  'raw_red_mean':raw_reds,'raw_green_mean': raw_greens,'raw_blue_mean': raw_blues,'raw_nir_mean': raw_nirs})\n",
    "\n",
    "# Write the full results to csv using the pandas library. \n",
    "df.to_csv('normalized_training_data.csv',encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
