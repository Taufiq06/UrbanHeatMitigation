{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Workflow: Geocode addresses from the ground truth data\n",
    "Purpose: Geocode the training data rooftop addresses using Google geocoding API. The latitude & longitude of the roofs will be used to search for their geometries from Microsoft footprints data.   \n",
    "<br>\n",
    "*Date: 10/31/2019*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/opt/caffe/python', '/opt/caffe2/build', '/data/home/peter/notebooks/urban_heat', '/anaconda/envs/py36/lib/python36.zip', '/anaconda/envs/py36/lib/python3.6', '/anaconda/envs/py36/lib/python3.6/lib-dynload', '/anaconda/envs/py36/lib/python3.6/site-packages', '/anaconda/envs/py36/lib/python3.6/site-packages/IPython/extensions', '/data/home/peter/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import itertools\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "#\n",
    "import numpy as np\n",
    "import shapely\n",
    "from shapely.geometry import shape, Point\n",
    "from shapely.geometry import mapping, Polygon\n",
    "# import cartopy\n",
    "import geojson\n",
    "import fiona\n",
    "import h5py\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gdal\n",
    "from glob import glob\n",
    "\n",
    "import jenkspy\n",
    "\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "from numpy import mean\n",
    "\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "import time\n",
    "\n",
    "print (sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set your input file here\n",
    "input_filename = \"ground_truth.csv\"\n",
    "\n",
    "# Read the data to a Pandas Dataframe\n",
    "data = pd.read_csv(input_filename, encoding='utf8')\n",
    "# Formatting the input address column so that it can be interpreted easily by the geocoding API\n",
    "data['Address'] = data['Name'] +', '+ data['Street Address'] +', '+ data['City'] + ', ' + data['State'] + ' ' + data['Zip']  # assigned to a column\n",
    "\n",
    "# Specify the column name in your input data that contains addresses here\n",
    "address_column_name = \"Address\"\n",
    "\n",
    "# Form a list of addresses for geocoding:\n",
    "# Make a big list of all of the addresses to be processed.\n",
    "addresses = data[address_column_name].tolist()\n",
    "addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------\tFUNCTION DEFINITIONS ------------------------\n",
    "\n",
    "def get_google_results(address, api_key=API_KEY, return_full_response=False):\n",
    "\n",
    "    # Set up your Geocoding url\n",
    "    geocode_url = \"https://maps.googleapis.com/maps/api/geocode/json?address={}\".format(address) + \"&key={}\".format(api_key)\n",
    "        \n",
    "    # Ping google for the reuslts:\n",
    "    results = requests.get(geocode_url)\n",
    "    # Results will be in JSON format - convert to dict using requests functionality\n",
    "    results = results.json()\n",
    "    \n",
    "    # if there's no results or an error, return empty results.\n",
    "    if len(results['results']) == 0:\n",
    "        output = {\n",
    "            \"formatted_address\" : None,\n",
    "            \"latitude\": None,\n",
    "            \"longitude\": None,\n",
    "            \"accuracy\": None,\n",
    "            \"google_place_id\": None,\n",
    "            \"type\": None,\n",
    "            \"postcode\": None\n",
    "        }\n",
    "    else:    \n",
    "        answer = results['results'][0]\n",
    "        output = {\n",
    "            \"formatted_address\" : answer.get('formatted_address'),\n",
    "            \"latitude\": answer.get('geometry').get('location').get('lat'),\n",
    "            \"longitude\": answer.get('geometry').get('location').get('lng'),\n",
    "            \"accuracy\": answer.get('geometry').get('location_type'),\n",
    "            \"viewport\": answer.get('geometry').get('viewport'),\n",
    "            \"bounds\": answer.get('geometry').get('bounds'),\n",
    "            \"google_place_id\": answer.get(\"place_id\"),\n",
    "            \"type\": \",\".join(answer.get('types')),\n",
    "            \"postcode\": \",\".join([x['long_name'] for x in answer.get('address_components') \n",
    "                                  if 'postal_code' in x.get('types')])\n",
    "        }\n",
    "        \n",
    "    # Append some other details:    \n",
    "    output['input_string'] = address\n",
    "    output['number_of_results'] = len(results['results'])\n",
    "    output['status'] = results.get('status')\n",
    "    if return_full_response is True:\n",
    "        output['response'] = results\n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supply the rooftop addresses and geocode them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapted from python batch geocoding.py by Shane Lynn\n",
    "logger = logging.getLogger(\"root\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# create console handler\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "#------------------ CONFIGURATION -------------------------------\n",
    "\n",
    "# Set your Google API key here. \n",
    "API_KEY = 'put your API key here'\n",
    "# Backoff time sets how many minutes to wait between google pings when your API limit is hit\n",
    "BACKOFF_TIME = 30\n",
    "\n",
    "# the output file containig the geocoded addresses\n",
    "output_filename = \"geocded_addresses.csv\"\n",
    "\n",
    "# Return Full Google Results? If True, full JSON results from Google are included in output\n",
    "RETURN_FULL_RESULTS = False\n",
    "\n",
    "# Create a list to hold results\n",
    "results = []\n",
    "# Go through each address in turn\n",
    "for address in addresses:\n",
    "    # While the address geocoding is not finished:\n",
    "    geocoded = False\n",
    "    while geocoded is not True:\n",
    "        # Geocode the address with google\n",
    "        try:\n",
    "            geocode_result = get_google_results(address, API_KEY, return_full_response=RETURN_FULL_RESULTS)\n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            logger.error(\"Major error with {}\".format(address))\n",
    "            logger.error(\"Skipping!\")\n",
    "            geocoded = True\n",
    "            \n",
    "        # If we're over the API limit, backoff for a while and try again later.\n",
    "        if geocode_result['status'] == 'OVER_QUERY_LIMIT':\n",
    "            logger.info(\"Hit Query Limit! Backing off for a bit.\")\n",
    "            time.sleep(BACKOFF_TIME * 60) # sleep for 30 minutes\n",
    "            geocoded = False\n",
    "        else:\n",
    "            if geocode_result['status'] != 'OK':\n",
    "                logger.warning(\"Error geocoding {}: {}\".format(address, geocode_result['status']))\n",
    "            logger.debug(\"Geocoded: {}: {}\".format(address, geocode_result['status']))\n",
    "            results.append(geocode_result)           \n",
    "            geocoded = True\n",
    "\n",
    "    # Print status every 100 addresses\n",
    "    if len(results) % 100 == 0:\n",
    "    \tlogger.info(\"Completed {} of {} address\".format(len(results), len(addresses)))\n",
    "            \n",
    "    # Every 500 addresses, save progress to file(in case of a failure so you have something!)\n",
    "    if len(results) % 500 == 0:\n",
    "        pd.DataFrame(results).to_csv(\"{}_bak\".format(output_filename))\n",
    "\n",
    "# All done\n",
    "logger.info(\"Finished geocoding all addresses\")\n",
    "# Write the full results to csv using the pandas library.\n",
    "pd.DataFrame(results).to_csv(output_filename, encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for matching footprints from Microsoft footprints data\n",
    "`Two way to achieve that. Either search from Geojson files uploaded to the VM or search from the DL vector product that contains all the Microsoft footprints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your input file containing the geocoded addresses\n",
    "input_filename = \"geocded_addresses.csv\"\n",
    "\n",
    "# Read the data to a Pandas Dataframe\n",
    "df = pd.read_csv(input_filename, encoding='utf8')\n",
    "\n",
    "addresses= df[['longitude','latitude','input_string','Warranty Start Date','Twelve Months','Solar-Initial']].apply(tuple, axis=1)\n",
    "addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download footprints directly from geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading footprints from California in this example\n",
    "with open('/data/phase_i/microsoft_footprints/California.geojson') as f:\n",
    "    js = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Y, X in addresses.iteritems():\n",
    "    print('Searching matching polygon for:', X[0],X[1])\n",
    "    # construct point based on lat/long returned by geocoder\n",
    "    point = Point(X[0],X[1])\n",
    "\n",
    "    # check each polygon to see if it contains the point\n",
    "    for feature in js['features']:\n",
    "        polygon = shape(feature['geometry'])\n",
    "        if polygon.contains(point):\n",
    "            print ('Found containing polygon:', feature)\n",
    "\n",
    "            # Define a polygon feature geometry with one attribute\n",
    "            schema = {\n",
    "                'geometry': 'Polygon',\n",
    "                'properties': {'id': 'int'},\n",
    "            }\n",
    "\n",
    "            # Write a new Shapefile\n",
    "            with fiona.open(data_root+'footprint_shapes/george'+str(X[1])+','+str(X[0])+'_msfootprint.shp', 'w', 'ESRI Shapefile', schema) as c:\n",
    "                ## If there are multiple geometries, put the \"for\" loop here\n",
    "                c.write({\n",
    "                    'geometry': mapping(polygon),\n",
    "                    'properties': {'id': 123},\n",
    "                })\n",
    "            #As soon as you find the matching polygon, break out of the loop\n",
    "            break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Download footprints from DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microsoft_footprints  = FeatureCollection('put the DL vector product id here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roof_add=[]\n",
    "foot_path = []\n",
    "start_dt=[]\n",
    "end_dt=[]\n",
    "in_solar = []\n",
    "# thr_solar = []\n",
    "lat=[]\n",
    "long=[]\n",
    "bld_shp = []\n",
    "\n",
    "cnt=0\n",
    "for Y, X in addresses.iteritems():\n",
    "    cnt=cnt+1\n",
    "    print(cnt)\n",
    "    print ('Searching matching polygon for:', X[0],X[1])\n",
    "    # construct point based on lat/long returned by geocoder\n",
    "    point = Point(X[0],X[1])\n",
    "    buf = point.buffer(0.00001)\n",
    "    buf = mapping(buf)\n",
    "    \n",
    "    s = dl.Vector().search_features('a35126a241bd022c026e96ab9fe5e0ea23967d08:USBuildingFootprints', geometry=buf, query_expr=None, query_limit=None,)\n",
    "\n",
    "    for feature in s:\n",
    "        att = feature.attributes\n",
    "        polygon = shape(att['geometry'])\n",
    "        \n",
    "        # Define a polygon feature geometry with one attribute\n",
    "        schema = {\n",
    "            'geometry': 'Polygon',\n",
    "            'properties': {'id': 'int'},\n",
    "            }\n",
    "\n",
    "        # Write a new Shapefile\n",
    "        with fiona.open(data_root+'footprint_shapes/george/'+str(X[1])+','+str(X[0])+'_msfootprint.shp', 'w', 'ESRI Shapefile', schema) as c:\n",
    "            ## If there are multiple geometries, put the \"for\" loop here\n",
    "            c.write({\n",
    "                'geometry': mapping(polygon),\n",
    "                'properties': {'id': 123},\n",
    "            })\n",
    "        \n",
    "        roof_add.append(X[2])\n",
    "        foot_path.append(data_root+'footprint_shapes/george/'+str(X[1])+','+str(X[0])+'_msfootprint.shp')\n",
    "        start_dt.append(X[3])\n",
    "        end_dt.append(X[4])\n",
    "        in_solar.append(X[5])\n",
    "#         thr_solar.append(X[6])\n",
    "        lat.append(X[1])\n",
    "        long.append(X[0])\n",
    "        bld_shp.append(polygon)\n",
    "        \n",
    "        break\n",
    "\n",
    "# store the results to a pandas library.\n",
    "df = pd.DataFrame({'roof_add': roof_add, 'footprint_path': foot_path, 'footprint_shapes': bld_shp, 'longitude': long,'latitude': lat, \n",
    "                   'start_date': start_dt,'end_date': end_dt,'Solar-Initial': in_solar})\n",
    "\n",
    "# Write the full results to csv using the pandas library. \n",
    "df.to_csv('footprints_data.csv',encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
