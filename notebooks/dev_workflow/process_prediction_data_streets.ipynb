{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Workflow: Get NAIP imageries from given addresses\n",
    "Purpose: The geometries of the streets are used to acquire NAIP imagery for all the streets within the AOI. The mean band values (R, G, B, NIR) for each streets are saved and used for subsequent operation. \n",
    "<br>\n",
    "*Date: 11-09-2019*\n",
    "<br>\n",
    "*Author: Taufiq Rashid*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import itertools\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "#\n",
    "import numpy as np\n",
    "import shapely\n",
    "from shapely.geometry import shape, Point\n",
    "from shapely.geometry import mapping, Polygon\n",
    "# import cartopy\n",
    "import geojson\n",
    "import fiona\n",
    "import gdal\n",
    "import h5py\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "# import ogr, gdal\n",
    "from glob import glob\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from numpy import mean\n",
    "\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "import descarteslabs as dl\n",
    "from descarteslabs.vectors import FeatureCollection\n",
    "\n",
    "print (sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shape(place_shapefile):\n",
    "    c = fiona.open(place_shapefile)\n",
    "    pol = c.next()\n",
    "    shape = {}\n",
    "    shape['type'] = pol['type']\n",
    "    shape['properties'] = pol['properties']\n",
    "    shape['geometry'] = {}\n",
    "    shape['geometry']['type'] = 'Polygon'  # pol['geometry']['type']\n",
    "    shape['geometry']['coordinates'] = [[]]\n",
    "    # if MultiPolygon (e.g., city='kampala')\n",
    "    if (len(pol['geometry']['coordinates'])>1):\n",
    "        # identify largest single polygon\n",
    "#         print(\"MultiPolygon\", len(pol['geometry']['coordinates']))\n",
    "        p_argmax = 0 \n",
    "        pn_max = 0\n",
    "        for p in range(len(pol['geometry']['coordinates'])):\n",
    "            pn = len(pol['geometry']['coordinates'][p][0])\n",
    "            if pn>pn_max:\n",
    "                p_argmax = p\n",
    "                pn_max = pn\n",
    "#             print(p, pn, p_argmax, pn_max )\n",
    "        # make largest polygon the only polygon, move other polys to a backup variable \n",
    "        polygon = pol['geometry']['coordinates'][p_argmax]\n",
    "    else:\n",
    "#         print('simple polygon')\n",
    "        polygon = pol['geometry']['coordinates']\n",
    "       \n",
    "    xmin =  180\n",
    "    xmax = -180\n",
    "    ymin =  90\n",
    "    ymax = -90\n",
    "    for x,y in polygon[0]:\n",
    "        xmin = xmin if xmin < x else x\n",
    "        xmax = xmax if xmax > x else x\n",
    "        ymin = ymin if ymin < y else y\n",
    "        ymax = ymax if ymax > y else y\n",
    "        shape['geometry']['coordinates'][0].append([x,y])\n",
    "    shape['bbox'] = [xmin,ymin,xmax,ymax]\n",
    "    \n",
    "    return shape\n",
    "\n",
    "\n",
    "import itertools\n",
    "from multiprocessing import Process, cpu_count\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#\n",
    "# CONFIG\n",
    "#\n",
    "MAX_POOL_PROCESSES=cpu_count()-1\n",
    "MAX_THREADPOOL_PROCESSES=16\n",
    "\n",
    "def map_with_pool(map_function,args_list,max_processes=MAX_POOL_PROCESSES):\n",
    "  pool=Pool(processes=min(len(args_list),max_processes))\n",
    "  return _run_pool(pool,map_function,args_list)\n",
    "\n",
    "\n",
    "def map_with_threadpool(map_function,args_list,max_processes=MAX_THREADPOOL_PROCESSES):\n",
    "  pool=ThreadPool(processes=min(len(args_list),max_processes))\n",
    "  return _run_pool(pool,map_function,args_list)\n",
    "\n",
    "\n",
    "def map_sequential(map_function,args_list,print_args=False,noisy=False,**dummy_kwargs):\n",
    "  if noisy:\n",
    "    print('multiprocessing(test):')\n",
    "  out=[]\n",
    "  for i,args in enumerate(args_list):\n",
    "      if noisy: \n",
    "        print('\\t{}...'.format(i))\n",
    "      if print_args:\n",
    "        print('\\t{}'.format(args))\n",
    "      out.append(map_function(args))\n",
    "  if noisy: \n",
    "    print('-'*25)\n",
    "  return out\n",
    "\n",
    "def simple(function,args_list,join=True):\n",
    "  procs=[]\n",
    "  for args in args_list:\n",
    "      proc=Process(\n",
    "          target=function, \n",
    "          args=args)\n",
    "      procs.append(proc)\n",
    "      proc.start()\n",
    "  if join:\n",
    "    for proc in procs:\n",
    "        proc.join()\n",
    "  return procs\n",
    "\n",
    "class MPList():\n",
    "    #\n",
    "    # POOL TYPES\n",
    "    #\n",
    "    POOL='pool'\n",
    "    THREAD='threading'\n",
    "    SEQUENTIAL='sequential'\n",
    "    \n",
    "\n",
    "    #\n",
    "    # PUBLIC\n",
    "    #\n",
    "    def __init__(self,pool_type=None,max_processes=None,jobs=None):\n",
    "        self.pool_type=pool_type or self.POOL\n",
    "        self.max_processes=max_processes\n",
    "        self.jobs=jobs or []\n",
    "\n",
    "        \n",
    "    def append(self,target,*args,**kwargs):\n",
    "        self.jobs.append((target,)+(args,)+(kwargs,))\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "        self.start_time=datetime.now()\n",
    "        map_func,self.max_processes=self._map_func_max_processes()\n",
    "        out=map_func(self._target,self.jobs,max_processes=self.max_processes)\n",
    "        self.end_time=datetime.now()\n",
    "        self.duration=str(self.end_time-self.start_time)\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.jobs)\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # INTERNAL\n",
    "    #    \n",
    "    def _map_func_max_processes(self):\n",
    "        if self.pool_type==MPList.THREAD:\n",
    "            map_func=map_with_threadpool\n",
    "            max_processes=self.max_processes or MAX_THREADPOOL_PROCESSES\n",
    "        elif self.pool_type==MPList.SEQUENTIAL:\n",
    "            map_func=map_sequential\n",
    "            max_processes=False\n",
    "        else:\n",
    "            map_func=map_with_pool\n",
    "            max_processes=self.max_processes or MAX_POOL_PROCESSES\n",
    "        return map_func, max_processes\n",
    "        \n",
    "        \n",
    "    def _target(self,args):\n",
    "        target,args,kwargs=args\n",
    "        return target(*args,**kwargs)\n",
    "        \n",
    "    \n",
    "\n",
    "#\n",
    "# INTERNAL METHODS\n",
    "#\n",
    "def _stop_pool(pool,success=True):\n",
    "  pool.close()\n",
    "  pool.join()\n",
    "  return success\n",
    "\n",
    "\n",
    "def _map_async(pool,map_func,objects):\n",
    "  try:\n",
    "    return pool.map_async(map_func,objects)\n",
    "  except KeyboardInterrupt:\n",
    "    print(\"Caught KeyboardInterrupt, terminating workers\")\n",
    "    pool.terminate()\n",
    "    return False\n",
    "  else:\n",
    "    print(\"Failure\")\n",
    "    return _stop_pool(pool,False)\n",
    "\n",
    "\n",
    "def _run_pool(pool,map_function,args_list):\n",
    "  out=_map_async(pool,map_function,args_list)\n",
    "  _stop_pool(pool)\n",
    "  return out.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_dict_decorator(func):\n",
    "    def decorator(arg_dict):\n",
    "        return func(**arg_dict)\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@arg_dict_decorator\n",
    "def calc_bands(type,properties,geometry):    \n",
    "    attempts = 0\n",
    "\n",
    "    while attempts < 2:\n",
    "        cnt = (properties['cnt'])\n",
    "        rf_no = cnt\n",
    "        img_id = - 1\n",
    "\n",
    "        try:\n",
    "            polygon = shape(geometry)\n",
    "\n",
    "            scenes, ctx = dl.scenes.search(geometry, products=product, start_datetime='2009-01-01', \n",
    "                                           end_datetime='2009-12-31', limit=None)                            \n",
    "\n",
    "            for scene in scenes:\n",
    "\n",
    "                img_id = img_id + 1\n",
    "\n",
    "                naip_data = scene.ndarray(bands=\"red green blue nir\", ctx=ctx.assign(resolution=1),mask_alpha=False)\n",
    "                red = naip_data[0]\n",
    "                red = red.astype(float)\n",
    "\n",
    "                green = naip_data[1]\n",
    "                green = green.astype(float)\n",
    "\n",
    "                blue = naip_data[2]\n",
    "                blue = blue.astype(float)\n",
    "\n",
    "                nir = naip_data[3]\n",
    "                nir = nir.astype(float)\n",
    "\n",
    "                arr = [red,green,blue,nir]\n",
    "\n",
    "                flat_arr = []\n",
    "                # flattened array of tuples\n",
    "                flat_list = zip(*map(lambda x:x.flatten(),arr))\n",
    "                for i in flat_list:\n",
    "                    flat_arr.append(i)   \n",
    "\n",
    "                selected_pixels=[]\n",
    "                # remove blank pixels and normalize for scenes\n",
    "                for pixels in flat_arr:\n",
    "                    if pixels[0] != 0 or pixels[1] != 0 or pixels[2] != 0 or pixels[3] != 0:\n",
    "                        selected_pixels.append(pixels)\n",
    "\n",
    "                # raw band values        \n",
    "                raw_red_b = []\n",
    "                raw_green_b = []\n",
    "                raw_blue_b = []\n",
    "                raw_nir_b = []\n",
    "\n",
    "                for pixels in selected_pixels:\n",
    "                    raw_red_b.append(pixels[0]) \n",
    "                    raw_green_b.append(pixels[1])\n",
    "                    raw_blue_b.append(pixels[2])\n",
    "                    raw_nir_b.append(pixels[3])\n",
    "\n",
    "                # calculate the mean values for all the bands from this list\n",
    "                raw_red_mean=mean(raw_red_b)\n",
    "                raw_green_mean=mean(raw_green_b)\n",
    "                raw_blue_mean=mean(raw_blue_b)\n",
    "                raw_nir_mean=mean(raw_nir_b)\n",
    "\n",
    "                total_pixel = len(selected_pixels) # calculate the size of the roof               \n",
    "\n",
    "                imgs.append(img_id)\n",
    "                roofs.append(rf_no)\n",
    "                footprint_shapes.append(polygon)\n",
    "\n",
    "                total_pixels.append(total_pixel)              \n",
    "\n",
    "                raw_reds.append(raw_red_mean)\n",
    "                raw_greens.append(raw_green_mean)\n",
    "                raw_blues.append(raw_blue_mean)\n",
    "                raw_nirs.append(raw_nir_mean)\n",
    "            break\n",
    "        except Exception as e:\n",
    "#             print ('some type of error at count ', cnt)\n",
    "#             print (e)\n",
    "            attempts += 1\n",
    "            if attempts == 2:\n",
    "                print('unsuccessfull at count', cnt)\n",
    "                unsuc_rfs.append(rf_no)\n",
    "            else:\n",
    "                time.sleep(2)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate band values for all roofs in LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LA_official_city_streets_2016.geojson') as f:\n",
    "    js = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arg_list = js['features']\n",
    "print(len(arg_list))\n",
    "\n",
    "cnt = -1\n",
    "\n",
    "for feat in arg_list:\n",
    "    cnt = cnt + 1\n",
    "    feat['properties']['cnt'] = cnt\n",
    "    \n",
    "# arg_list = arg_list[0:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "product = u'usda:naip:rgbn:v1'\n",
    "\n",
    "roofs = []\n",
    "imgs = []\n",
    "footprint_shapes=[]\n",
    "total_pixels = []\n",
    "\n",
    "raw_reds = []\n",
    "raw_greens = []\n",
    "raw_blues = []\n",
    "raw_nirs = []\n",
    "\n",
    "unsuc_rfs = []\n",
    "\n",
    "%time out = map_with_threadpool(calc_bands,arg_list,max_processes=64)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('finished multiprocessing') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# store the results to a pandas library.\n",
    "df = pd.DataFrame({ 'roof_no': roofs, 'img_id':imgs, 'footprint_shapes':footprint_shapes,'total_pixels': total_pixels,\n",
    "                  'raw_red_mean':raw_reds,'raw_green_mean': raw_greens,'raw_blue_mean': raw_blues,'raw_nir_mean': raw_nirs})\n",
    "\n",
    "# Write the full results to csv using the pandas library. \n",
    "df.to_csv('band_values_NAIP_LA_city_rd_2009_10-28.csv',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_count = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
