{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Workflow: Calculate precision scores \n",
    "Purpose: Calculate precision scores for duplicate imagery\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "579e960c-3156-49fa-be08-11fe67948ead"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,5,6,9]\n",
    "import numpy as np\n",
    "print(np.mean(a))\n",
    "print(np.std(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svfig(path):\n",
    "    plt.savefig(path, dpi=None, facecolor='w', edgecolor='w',\n",
    "        orientation='portrait', papertype=None, format=None,\n",
    "        transparent=False, bbox_inches=None, pad_inches=1, \n",
    "        frameon=None, metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: only 2 instance duplicates, 1: all duplicates (max and min from each)\n",
    "def group_dupls(df,overlap_code=0):\n",
    "    if overlap_code==0:\n",
    "        dfr = df[df['ScoredLabels']==2].copy()\n",
    "        dfr['range'] = dfr['max']-dfr['min']\n",
    "        print(dfr['range'].count())\n",
    "        return dfr\n",
    "    elif overlap_code == 1:\n",
    "        df = df[df['ScoredLabels']>1].copy()\n",
    "        df['range'] = df['max']-df['min']\n",
    "        print(df['range'].count())\n",
    "        return df\n",
    "    else:\n",
    "        print ('??')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "# Set figure width to 12 and height to 9\n",
    "fig_size[0] = 14\n",
    "fig_size[1] = 10\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_all_09 = [\n",
    "    'pred_LA_2009_linear_sika+gg+ss+smote_imitgg_norm_rm-hi-alb_9-14.csv',\n",
    "    'pred_LA_2009_linear_sika+gg+ss+smote_imitgg_org_rm-hi-alb_9-14.csv',\n",
    "               ]\n",
    "\n",
    "datasets_all =  [\n",
    "#     'pred_LA_2014_boosted_sika+gg_imitgg_norm_9-3.csv',\n",
    "               ]\n",
    "\n",
    "datasets_decision_09 = [\n",
    "#     'pred_LA_2009_decision_sika+gg+ss+smote_imitgg_norm_rm-outlr_9-16.csv',\n",
    "#     'pred_LA_2009_decision_sika+gg+ss+smote_imitgg_norm_rm-outlr_9-16_v2.csv',\n",
    "#     'pred_LA_cnty_2009_train-50m_pred-rf-norm_10-23.csv',\n",
    "                    ]\n",
    "\n",
    "datasets_decision = [\n",
    "    'pred_LA_cnty_2018_train-50m_pred-rf-norm_10-23.csv',\n",
    "#     'pred_LA_2014_decision_sika+gg_imitgg_norm-all_9-9.csv',\n",
    "#     'pred_LA_2016_decision_sika+gg_imitgg_norm-all_9-9.csv'\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stat for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset in datasets_all_09:\n",
    "    print (dataset)\n",
    "    df_pre = pd.read_csv(dataset)\n",
    "    df_pre = df_pre[df_pre.gg_prediction != 0]\n",
    "    df_pre['Scored Labels'] = df_pre['Scored Labels']\n",
    "    df = df_pre[['footprint_shapes','gg_prediction']]   \n",
    "    df['ScoredLabels']=df_pre['Scored Labels']\n",
    "   \n",
    "    dfvd = pd.read_csv(dataset)  \n",
    "    dfv = dfvd.dropna()\n",
    "    dfv['Scored Labels']=dfv['Scored Labels']\n",
    "    dfv['ScoredLabels']=dfv['Scored Labels']\n",
    "    p = dfv['ScoredLabels']\n",
    "    o = dfv['gg_prediction']\n",
    "    dfv['residual'] = p-o\n",
    "    dfv['AME'] = abs(p-o)\n",
    "\n",
    "    rmse = sqrt(mean_squared_error(o, p))\n",
    "#     print (rmse)\n",
    "\n",
    "    dfvd = dfv[['gg_prediction', 'ScoredLabels', 'residual', 'AME']]\n",
    "    \n",
    "    dfv = dfvd.dropna()\n",
    "    f_low_alb = dfv['gg_prediction']<0.2\n",
    "    low_alb = dfv[f_low_alb]\n",
    "    pl = low_alb['ScoredLabels']\n",
    "    ol = low_alb['gg_prediction']\n",
    "    low_alb.describe()\n",
    "\n",
    "    rmsel = sqrt(mean_squared_error(ol, pl))\n",
    "#     print (rmsel)\n",
    "\n",
    "    low_albd = low_alb[['gg_prediction', 'ScoredLabels', 'residual', 'AME']]  \n",
    "    \n",
    "    dfv = dfvd.dropna()\n",
    "    f_high_alb = dfv['gg_prediction']>0.4\n",
    "    high_alb = dfv[f_high_alb]\n",
    "    ph = high_alb['ScoredLabels']\n",
    "    oh = high_alb['gg_prediction']\n",
    "    high_alb.describe()\n",
    "\n",
    "    rmseh = sqrt(mean_squared_error(oh, ph))\n",
    "#     print (rmseh)\n",
    "\n",
    "    high_albd = high_alb[['gg_prediction', 'ScoredLabels', 'residual', 'AME']]\n",
    "    \n",
    "    ame = dfv.AME.mean()\n",
    "    amel = low_alb.AME.mean()\n",
    "    ameh = high_alb.AME.mean()\n",
    "\n",
    "#     print ()\n",
    "    print (\"RMSE: \", str(round(rmse, 4)), ' (entire valdiation set, all expected albedos)')\n",
    "    print (\"RMSE: \", str(round(rmsel, 4)), ' (low expected albedos, <0.2)')\n",
    "    print (\"RMSE: \", str(round(rmseh, 4)), ' (high expected albedos, >0.4)')\n",
    "#     print ()\n",
    "    print (\"MAE:  \", str(round(ame, 4)), ' (entire valdiation set, all expected albedos)')\n",
    "#     print (\"MAE:  \", str(round(amel, 4)), ' (low expected albedos, <0.2)')\n",
    "#     print (\"MAE:  \", str(round(ameh, 4)), ' (high expected albedos, >0.4)')\n",
    "#     print ()\n",
    "    \n",
    "    df['min']=df['ScoredLabels']\n",
    "    df['max']=df['ScoredLabels']  \n",
    "    \n",
    "#     a = df.shape[0]\n",
    "#     b = df.footprint_shapes.value_counts()\n",
    "\n",
    "#     print (\"Unique Bldg Footprints:\", str(len(b)))\n",
    "#     print ()\n",
    "#     print (\"Predictions:\", str(a)+\"  \"+compareA)\n",
    "#     print ()\n",
    "#     print (\"Duplicates:\", str(a-len(b))+\"  \"+compareA)\n",
    "#     print (\"Duplicates: %\", str(round((1-(len(b)/a))*100, 2))+\"  \"+compareA)\n",
    "    \n",
    "    aggfunc = {'ScoredLabels':'count', 'min':lambda x: x.min(), 'max':lambda x: x.max()}\n",
    "    df_grp  = df.groupby(['footprint_shapes']).agg(aggfunc).reset_index().copy()\n",
    "#     print(df_grp.shape[0])\n",
    "    \n",
    "    df_grp = group_dupls(df_grp)\n",
    "    \n",
    "#     df_r = df_grp.drop(['min','max'],1)\n",
    "#     df_r.describe()\n",
    "    mrange = df_grp.range.mean()\n",
    "    print (\"MAE for duplicates: \", str(round(mrange, 4)))\n",
    "    \n",
    "    p = df_grp['max']\n",
    "    o = df_grp['min']\n",
    "\n",
    "    rmsed = sqrt(mean_squared_error(o, p))\n",
    "    print ('rmse for duplicates: ' + str(rmsed))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For predictions other than 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset in datasets_all:\n",
    "    print (dataset)\n",
    "    df_pre = pd.read_csv(dataset)\n",
    "    df_pre['Scored Labels'] = df_pre['Scored Labels']\n",
    "    df = df_pre[['footprint_shapes']]   \n",
    "    df['ScoredLabels']=df_pre['Scored Labels']\n",
    "   \n",
    "    \n",
    "    df['min']=df['ScoredLabels']\n",
    "    df['max']=df['ScoredLabels']  \n",
    "    \n",
    "    aggfunc = {'ScoredLabels':'count', 'min':lambda x: x.min(), 'max':lambda x: x.max()}\n",
    "    df_grp  = df.groupby(['footprint_shapes']).agg(aggfunc).reset_index().copy()\n",
    "#     print(df_grp.shape[0])\n",
    "    \n",
    "    df_grp = group_dupls(df_grp)\n",
    "    \n",
    "#     df_r = df_grp.drop(['min','max'],1)\n",
    "#     df_r.describe()\n",
    "    mrange = df_grp.range.mean()\n",
    "    print (\"MAE for duplicates: \", str(round(mrange, 4)))\n",
    "    \n",
    "    p = df_grp['max']\n",
    "    o = df_grp['min']\n",
    "\n",
    "    rmsed = sqrt(mean_squared_error(o, p))\n",
    "    print ('rmse for duplicates: ' + str(rmsed))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stat for Decision Forest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets_decision_09:\n",
    "    print (dataset)\n",
    "    df_pre = pd.read_csv(dataset)\n",
    "    df_pre = df_pre[df_pre.gg_prediction != 0]\n",
    "    df_pre['Scored Labels'] = df_pre['Scored Label Mean']\n",
    "    df = df_pre[['footprint_shapes','gg_prediction']]   \n",
    "    df['ScoredLabels']=df_pre['Scored Labels']\n",
    "   \n",
    "    dfvd = pd.read_csv(dataset)  \n",
    "    dfv = dfvd.dropna()\n",
    "    dfv['Scored Labels']=dfv['Scored Label Mean']\n",
    "    dfv['ScoredLabels']=dfv['Scored Labels']\n",
    "    p = dfv['ScoredLabels']\n",
    "    o = dfv['gg_prediction']\n",
    "    dfv['residual'] = p-o\n",
    "    dfv['AME'] = abs(p-o)\n",
    "\n",
    "    rmse = sqrt(mean_squared_error(o, p))\n",
    "#     print (rmse)\n",
    "\n",
    "    dfvd = dfv[['gg_prediction', 'ScoredLabels', 'residual', 'AME']]\n",
    "    \n",
    "    dfv = dfvd.dropna()\n",
    "    f_low_alb = dfv['gg_prediction']<0.2\n",
    "    low_alb = dfv[f_low_alb]\n",
    "    pl = low_alb['ScoredLabels']\n",
    "    ol = low_alb['gg_prediction']\n",
    "    low_alb.describe()\n",
    "\n",
    "    rmsel = sqrt(mean_squared_error(ol, pl))\n",
    "#     print (rmsel)\n",
    "\n",
    "    low_albd = low_alb[['gg_prediction', 'ScoredLabels', 'residual', 'AME']]  \n",
    "    \n",
    "    dfv = dfvd.dropna()\n",
    "    f_high_alb = dfv['gg_prediction']>0.4\n",
    "    high_alb = dfv[f_high_alb]\n",
    "    ph = high_alb['ScoredLabels']\n",
    "    oh = high_alb['gg_prediction']\n",
    "    high_alb.describe()\n",
    "\n",
    "    rmseh = sqrt(mean_squared_error(oh, ph))\n",
    "#     print (rmseh)\n",
    "\n",
    "    high_albd = high_alb[['gg_prediction', 'ScoredLabels', 'residual', 'AME']]\n",
    "    \n",
    "    ame = dfv.AME.mean()\n",
    "    amel = low_alb.AME.mean()\n",
    "    ameh = high_alb.AME.mean()\n",
    "\n",
    "#     print ()\n",
    "    print (\"RMSE: \", str(round(rmse, 4)), ' (entire valdiation set, all expected albedos)')\n",
    "    print (\"RMSE: \", str(round(rmsel, 4)), ' (low expected albedos, <0.2)')\n",
    "    print (\"RMSE: \", str(round(rmseh, 4)), ' (high expected albedos, >0.4)')\n",
    "#     print ()\n",
    "    print (\"MAE:  \", str(round(ame, 4)), ' (entire valdiation set, all expected albedos)')\n",
    "#     print (\"MAE:  \", str(round(amel, 4)), ' (low expected albedos, <0.2)')\n",
    "#     print (\"MAE:  \", str(round(ameh, 4)), ' (high expected albedos, >0.4)')\n",
    "#     print ()\n",
    "    \n",
    "    df['min']=df['ScoredLabels']\n",
    "    df['max']=df['ScoredLabels']  \n",
    "    \n",
    "#     a = df.shape[0]\n",
    "#     b = df.footprint_shapes.value_counts()\n",
    "\n",
    "#     print (\"Unique Bldg Footprints:\", str(len(b)))\n",
    "#     print ()\n",
    "#     print (\"Predictions:\", str(a)+\"  \"+compareA)\n",
    "#     print ()\n",
    "#     print (\"Duplicates:\", str(a-len(b))+\"  \"+compareA)\n",
    "#     print (\"Duplicates: %\", str(round((1-(len(b)/a))*100, 2))+\"  \"+compareA)\n",
    "    \n",
    "    aggfunc = {'ScoredLabels':'count', 'min':lambda x: x.min(), 'max':lambda x: x.max()}\n",
    "    df_grp  = df.groupby(['footprint_shapes']).agg(aggfunc).reset_index().copy()\n",
    "#     print(df_grp.shape[0])\n",
    "    \n",
    "    df_grp = group_dupls(df_grp)\n",
    "    \n",
    "#     df_r = df_grp.drop(['min','max'],1)\n",
    "#     df_r.describe()\n",
    "    mrange = df_grp.range.mean()\n",
    "    print (\"MAE for duplicates: \", str(round(mrange, 4)))\n",
    "    \n",
    "    p = df_grp['max']\n",
    "    o = df_grp['min']\n",
    "\n",
    "    rmsed = sqrt(mean_squared_error(o, p))\n",
    "    print ('rmse for duplicates: ' + str(rmsed))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For predictions other than 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_LA_cnty_2018_train-50m_pred-rf-norm_10-23.csv\n",
      "439038\n",
      "MAE for duplicates:  0.1967\n",
      "rmse for duplicates: 0.3689749002220586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets_decision:\n",
    "    print (dataset)\n",
    "    df_pre = pd.read_csv(dataset)\n",
    "    df_pre['Scored Labels'] = df_pre['Scored Label Mean']\n",
    "    df = df_pre[['footprint_shapes']]   \n",
    "    df['ScoredLabels']=df_pre['Scored Labels']\n",
    "    \n",
    "    df['min']=df['ScoredLabels']\n",
    "    df['max']=df['ScoredLabels']  \n",
    "    \n",
    "    \n",
    "    aggfunc = {'ScoredLabels':'count', 'min':lambda x: x.min(), 'max':lambda x: x.max()}\n",
    "    df_grp  = df.groupby(['footprint_shapes']).agg(aggfunc).reset_index().copy()\n",
    "#     print(df_grp.shape[0])\n",
    "    \n",
    "    df_grp = group_dupls(df_grp)\n",
    "    \n",
    "    mrange = df_grp.range.mean()\n",
    "    print (\"MAE for duplicates: \", str(round(mrange, 4)))\n",
    "    \n",
    "    p = df_grp['max']\n",
    "    o = df_grp['min']\n",
    "\n",
    "    rmsed = sqrt(mean_squared_error(o, p))\n",
    "    print ('rmse for duplicates: ' + str(rmsed))\n",
    "    print ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
